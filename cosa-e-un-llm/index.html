<!DOCTYPE html><html lang="it"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Cosa è un LLM? - Chatbot</title><meta name="description" content="In breve LLM o Large Language Models sono modelli linguistici di grandi dimensioni di deep learning in grado di svolgere un’ampia gamma di compiti legati&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://rainkiller981.github.io/Ricerca_su_Chatbot/cosa-e-un-llm/"><link rel="alternate" type="application/atom+xml" href="https://rainkiller981.github.io/Ricerca_su_Chatbot/feed.xml"><link rel="alternate" type="application/json" href="https://rainkiller981.github.io/Ricerca_su_Chatbot/feed.json"><meta property="og:title" content="Cosa è un LLM?"><meta property="og:site_name" content="Chatbot"><meta property="og:description" content="In breve LLM o Large Language Models sono modelli linguistici di grandi dimensioni di deep learning in grado di svolgere un’ampia gamma di compiti legati&hellip;"><meta property="og:url" content="https://rainkiller981.github.io/Ricerca_su_Chatbot/cosa-e-un-llm/"><meta property="og:type" content="article"><link rel="preload" href="https://rainkiller981.github.io/Ricerca_su_Chatbot/assets/dynamic/fonts/jetbrainsmono/jetbrainsmono.woff2" as="font" type="font/woff2" crossorigin><link rel="stylesheet" href="https://rainkiller981.github.io/Ricerca_su_Chatbot/assets/css/style.css?v=c6cdfdcc5b3eecb6e6896ba6aa7de20a"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://rainkiller981.github.io/Ricerca_su_Chatbot/cosa-e-un-llm/"},"headline":"Cosa è un LLM?","datePublished":"2024-06-16T17:17","dateModified":"2024-06-18T23:19","description":"In breve LLM o Large Language Models sono modelli linguistici di grandi dimensioni di deep learning in grado di svolgere un’ampia gamma di compiti legati&hellip;","author":{"@type":"Person","name":"Andrea Pusceddu e Anna Maria Sanna","url":"https://rainkiller981.github.io/Ricerca_su_Chatbot/authors/andrea-pusceddu-e-anna-maria-sanna/"},"publisher":{"@type":"Organization","name":"Andrea Pusceddu e Anna Maria Sanna"}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body><div class="container"><header class="header"><div class="header__logo"><a class="logo" href="https://rainkiller981.github.io/Ricerca_su_Chatbot/">Chatbot</a></div><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu" aria-haspopup="true" aria-expanded="false"><span class="navbar__toggle-box"><span class="navbar__toggle-inner">Menu</span></span></button><ul class="navbar__menu"></ul></nav></header><main class="content"><article class="post"><header><h1 class="post__title">Cosa è un LLM?</h1><div class="post__meta"><time datetime="2024-06-16T17:17" class="post__date">domenica, 16 giugno 2024 </time><span class="post__author"><a href="https://rainkiller981.github.io/Ricerca_su_Chatbot/authors/andrea-pusceddu-e-anna-maria-sanna/" class="feed__author">Andrea Pusceddu e Anna Maria Sanna</a></span></div></header><div class="post__entry"><h3 id="in-breve">In breve</h3><p><em>LLM</em> o <em>Large Language Models</em> sono modelli linguistici di grandi dimensioni di deep learning in grado di svolgere un’ampia gamma di compiti legati all’elaborazione del linguaggio.</p><p></p><figure class="post__image"><img loading="lazy" src="https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/LLM.png" alt="Image description" width="926" height="300" sizes="(min-width: 920px) 703px, (min-width: 700px) calc(82vw - 35px), calc(100vw - 81px)" srcset="https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/LLM-xs.webp 300w, https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/LLM-sm.webp 480w, https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/LLM-md.webp 768w, https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/LLM-lg.webp 1024w"></figure><p></p><p><em>Rappresentazione schematica semplificata di un LLM</em></p><h3 id="addestramento">Addestramento</h3><p>I <em>LLM</em> vengono addestrati su una grande quantità e varietà di dati, che consente loro di imparare a riconoscere schemi complicati, concatenati e inoltre captare le sfumature linguistiche di quello che è il linguaggio umano e di mettere a punto i loro parametri. La maggior parte dei <em>LLM</em> viene addestrata su informazioni e dati ricavati grazie ad Internet, che comprendono fonti come articoli di Wikipedia e libri. Questi dati (in forma testuale) vengono dunque utilizzati per addestrare i nuovi <em>LLM</em>. Modelli come <em>Llama, GPT3.5/4 e GPT-J</em> vengono “allenati” tramite una modalità chiamata “modellazione linguistica causale”. In questo caso, l’obiettivo del modello è quello di prevedere il token di testo in base al contesto in cui ci si trova.</p><p></p><figure class="post__image"><img loading="lazy" src="https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/LLM2.png" alt="Image description" width="488" height="364" sizes="(min-width: 920px) 703px, (min-width: 700px) calc(82vw - 35px), calc(100vw - 81px)" srcset="https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/LLM2-xs.webp 300w, https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/LLM2-sm.webp 480w, https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/LLM2-md.webp 768w, https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/LLM2-lg.webp 1024w"></figure><p></p><p><em>Addestramento di un LLM</em></p><p>Esempio: se tu chiedi a <em>GPT4</em> <em>“Che cosa pensi della guerra?”</em> ti risponderà frasi come <em>“La guerra è un fenomeno complesso e devastante che ha accompagnato l’umanità per millenni. Le sue implicazioni sono vastissime e coprono una gamma di aspetti che vanno dal sociale, politico ed economico fino a quelli umanitari ed etici.”</em> andando poi ad elencarne caratteri aggiuntivi come aspetti negativi, possibili giustificazioni e considerazioni etiche e morali, senza averli richiesti.</p><p>L’addestramento dei modelli linguistici causali può essere molto impegnativo per la nostra macchina. Spesso sono necessarie notevoli risorse “computazionali”, tra cui migliaia di unità di elaborazione, per addestrare modelli di larga scala come <em>Llama</em> o <em>ChatGPT</em><sup class="footnote" id="fnref-1"><a href="https://wikia.schneedc.com/en/llm" rel="footnote">1</a></sup>.</p><h3 id="tokenization">Tokenization</h3><p>La <em>tokenizzazione</em> è un processo fondamentale dell’intelligenza generativa che prevede la suddivisione di un testo in singole unità denominate <em>token</em>. Questi <em>token</em> possono essere parole o anche caratteri, a seconda del livello di accuratezza richiesto per il compito da svolgere. Perché? Poiché le macchine non comprendono appieno un dato linguaggio, dobbiamo dunque convertire le parole in rappresentazioni numeriche. Tuttavia, separare le parole in base alle lettere dell’alfabeto è sostanzialmente inefficace, in quanto occorrono in media quattro <em>token</em> per una parola. D’altra parte, anche la separazione delle parole in base alla parola stessa è inefficiente, dato che per l’inglese sarebbe addirittura necessario un vocabolario composto da 171.476 unità. Per questo motivo, creiamo un modello di <em>tokenizer</em> di piccole dimensioni, addestrato tramite molteplici testi, per fornire la <em>tokenizzazione</em> più efficace con il vocabolario più piccolo (massima resa con il minimo sforzo).</p><p>Un <em>tokenizer</em> ha il compito di preparare gli input per un determinato modello. La maggior parte dei <em>tokenizer</em> è disponibile in due versioni: un’implementazione completa in python e un’implementazione “veloce” basata sulla libreria Rust Tokenizers. L’implementazione “veloce” consente una significativa accelerazione, in particolare quando si esegue la <em>tokenizzazione</em> in batch con metodi aggiuntivi per la mappatura tra la stringa originale (caratteri e parole) e lo spazio dei token (ad esempio, ottenendo l’indice del token che comprende un dato carattere o l’intervallo di caratteri corrispondenti a un dato token).</p><p></p><figure class="post__image"><img loading="lazy" src="https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/Token.png" alt="Image description" width="1400" height="788" sizes="(min-width: 920px) 703px, (min-width: 700px) calc(82vw - 35px), calc(100vw - 81px)" srcset="https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/Token-xs.webp 300w, https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/Token-sm.webp 480w, https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/Token-md.webp 768w, https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/Token-lg.webp 1024w"></figure><p></p><p><em>Rappresentazione grafica del funzionamento del processo del <strong>Tokenization</strong></em></p><p>Per esempio, quando si lavora con i modelli di intelligenza artificiale, spesso si vanno a creare dei dati di testo sotto forma di vettori numerici. In questo contesto, i <em>token</em> sono singole parole del testo e le caratteristiche del modello di apprendimento automatico corrispondono alle parole presenti nel vocabolario.</p><p>Di fatto la <em>tokenizzazione</em> non è un processo così semplice, in quanto comporta la definizione di ciò che costituisce una “parola”.</p><p>Esempio: la contrazione <strong>“aren’t”</strong> può essere trattata come un singolo token, scomposto in <strong>“aren/‘/t”</strong>, o <strong>“are/n’t”</strong>.</p><p>Lo schema di <em>tokenizzazione</em> scelto può avere un impatto significativo sui risultati dei vari modelli di intelligenza artificiale, quindi richiede un’attenta comprensione<sup class="footnote" id="fnref-1"><a href="none" rel="footnote">2</a></sup>.</p><h3 id="embeddings">Embeddings</h3><p>Gli <em>embeddings</em> sono rappresentazioni matematiche di dati come testi, immagini e audio, utilizzate da modelli di apprendimento automatico e algoritmi di ricerca semantica. Queste rappresentazioni trasformano oggetti in forme numeriche basate sulle loro caratteristiche e categorie.</p><p>L’input <em>tokenizzato</em> viene elaborato attraverso diversi livelli. Un primo livello assegna un vettore a ciascun token. A seconda dell’architettura, questo vettore può contenere o meno la posizione del token nella sequenza.</p><p><em>Embendding</em> dei token: Il primo vettore rappresenta il significato semantico del token, appreso durante l’addestramento. Serve come punto di partenza per la comprensione del modello. Gli <em>embeddings</em> che sono vicini fra loro sono considerabili come simili, utilizzando gli <em>embeddings</em> per questa funzione, l’algoritmo velocizza il processo di identificazione di parole simili fra loro, in quanto la distanza fra due vettori identifica la loro relazione (più piccola è la distanza, maggiore è la loro relazione)</p><p></p><figure class="post__image"><img loading="lazy" src="https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/Embeddings.jpg" alt="Image description" width="5388" height="2460" sizes="(min-width: 920px) 703px, (min-width: 700px) calc(82vw - 35px), calc(100vw - 81px)" srcset="https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/Embeddings-xs.webp 300w, https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/Embeddings-sm.webp 480w, https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/Embeddings-md.webp 768w, https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/Embeddings-lg.webp 1024w"></figure><p></p><p><em>Semplificazione del processo di embedding</em></p><p>Nelle <em>chatbot</em> e negli <em>LLM</em> il contesto che viene affidato ad ogni parola viene considerato un <em>embedding</em> in supporto alla parola stessa. In questo modo il significato di interi testi, chat e conversazioni può essere analizzato e studiato.</p><p>Esempi di <em>embeddings</em> testuali che vengono proposti da <em>OpenAi</em> per <em>ChatGPT</em> vengono utilizzati per funzioni come la ricerca, la classificazione e l’identificazione di anomalie.<sup class="footnote" id="fnref-1"><a href="https://www.cloudflare.com/it-it/learning/ai/what-are-embeddings/" rel="footnote"></a>3</sup></p><h3 id="decoder-layers">Decoder Layers</h3><p>I token passano attraverso il <em>Decoder</em>, che consiste in più strati identici (il numero può variare). Ogni strato contiene due sottolivelli principali.</p><ul><li>Il <em>Multi-Head self-attention sub-layer</em> che aiuta il modello a capire come le parole in una frase siano relazionate tra di loro e quale tipo di relazione abbiano, permettendo al modello di capirne il significato e il contesto.</li></ul><p>Esempio: Nella frase <strong>“Ho mangiato una mela”</strong>, il <em>sub-layer</em> mette in relazione fra di loro la parola <strong>“mangiato”</strong> come azione, la parola <strong>“mela”</strong> come oggetto dell’azione e la parola <strong>“ho”</strong> come soggetto dell’azione, <strong>“una”</strong> come relazione fra <strong>“mela”</strong> e <strong>“mangiato”</strong>.</p><ul><li>Il <em>Feed-forward network sub-layer</em> che raffina e affina la comprensione del testo inserito nel modello grazie a delle operazioni matematiche che distinguono pattern più complessi e connessioni tra parole per ottenere un significato più accurato.</li></ul><p>Esempio: Nella frase <strong>“Ho mangiato una mela”</strong> questo <em>sub-layer</em> permette al modello di capire il significato dell’azione che viene effettuata. Se per esempio l’utente richiede al chatbot <em>“Che cosa hai mangiato oggi?”</em> una delle possibili risposte sarà <em>“Ho mangiato una mela”</em>.</p><h3 id="output-generation">Output Generation</h3><p>Come ultimo step il modello utilizza la funzione <em>softmax</em>. Questa funzione converte i punteggi che indicano la probabilità che in quel modello ritenga necessario l’utilizzo di quel token in quella determinata situazione. Queste probabilità indicano la possibilità che ogni token sia la parola successiva nella sequenza di output.</p><p>Esempio: mettendo il caso in cui l’utente chiede al chatbot <em>“Che ore sono?”</em>, il chatbot attraverso la funzione <em>softmax</em> creerà la frase più adatta a quella situazione che potrebbe essere <em>“Sono le 20:34”</em> oppure <em>“Guardati da solo l’ora”</em>.</p><p></p><figure class="post__image"><img loading="lazy" src="https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/LLM3.jpg" alt="Image description" width="1400" height="700" sizes="(min-width: 920px) 703px, (min-width: 700px) calc(82vw - 35px), calc(100vw - 81px)" srcset="https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/LLM3-xs.webp 300w, https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/LLM3-sm.webp 480w, https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/LLM3-md.webp 768w, https://rainkiller981.github.io/Ricerca_su_Chatbot/media/posts/24/responsive/LLM3-lg.webp 1024w"></figure><p></p><p><em>Come funzionerebbe un LLM</em></p><p>Infine, il <em>LLM</em> produce un output selezionando i token. La scelta è influenzata da vari fattori che vedremo in seguito<sup class="footnote" id="fnref-1"><a href="https://wikia.schneedc.com/en/llm" rel="footnote">4</a></sup>.</p><p>Si suggeriscono i seguenti video:</p><ul><li><a href="https://www.youtube.com/watch?v=zKndCikg3R0">https://www.youtube.com/watch?v=zKndCikg3R0</a></li><li><a href="https://www.youtube.com/watch?v=zizonToFXDs">https://www.youtube.com/watch?v=zizonToFXDs</a></li></ul><h3 id="note">Note</h3><ol><li><sup class="footnote" id="fnref-1"><a href="https://wikia.schneedc.com/en/llm" rel="footnote"></a></sup><a href="https://wikia.schneedc.com/en/llm">https://wikia.schneedc.com/en/llm</a></li><li><sup class="footnote" id="fnref-1"><a href="none" rel="footnote"></a></sup>Ibidem.</li><li><sup class="footnote" id="fnref-1"><a href="https://www.cloudflare.com/it-it/learning/ai/what-are-embeddings/" rel="footnote"></a></sup><a href="https://www.cloudflare.com/it-it/learning/ai/what-are-embeddings/">https://www.cloudflare.com/it-it/learning/ai/what-are-embeddings/</a></li><li><sup class="footnote" id="fnref-1"><a href="https://wikia.schneedc.com/en/llm" rel="footnote"></a></sup><a href="https://wikia.schneedc.com/en/llm">https://wikia.schneedc.com/en/llm</a></li></ol></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on martedì, 18 giugno 2024</p><div class="post__share"></div></footer></article></main><footer class="footer"><div class="footer__inner"><div class="footer__copyright"><p>© 2024 Powered by Publii CMS :: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank" rel="noopener">Theme</a> ported by the <a href="https://getpublii.com/customization-service/" target="_blank" rel="noopener">Publii Team</a></p></div></div></footer></div><script defer="defer" src="https://rainkiller981.github.io/Ricerca_su_Chatbot/assets/js/scripts.min.js?v=74fad06980c30243d91d72c7c57fcdb8"></script><script>window.publiiThemeMenuConfig={mobileMenuMode:'overlay',animationSpeed:300,submenuWidth: 'auto',doubleClickTime:500,mobileMenuExpandableSubmenus:true,relatedContainerForOverlayMenuSelector:'.top'};</script><script>var images = document.querySelectorAll('img[loading]');
        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>