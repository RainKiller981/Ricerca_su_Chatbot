<!DOCTYPE html><html lang="it"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Software per la gestione dei Chatbot - Chatbot</title><meta name="description" content="In questa sezione si analizzano tutti i software utilizzati per la ricerca, ossia BackyardAI, TavernAI e OobaBooga."><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://rainkiller981.github.io/Ricerca_su_Chatbot/software-per-la-gestione-dei-chatbot/"><link rel="alternate" type="application/atom+xml" href="https://rainkiller981.github.io/Ricerca_su_Chatbot/feed.xml"><link rel="alternate" type="application/json" href="https://rainkiller981.github.io/Ricerca_su_Chatbot/feed.json"><meta property="og:title" content="Software per la gestione dei Chatbot"><meta property="og:site_name" content="Chatbot"><meta property="og:description" content="In questa sezione si analizzano tutti i software utilizzati per la ricerca, ossia BackyardAI, TavernAI e OobaBooga."><meta property="og:url" content="https://rainkiller981.github.io/Ricerca_su_Chatbot/software-per-la-gestione-dei-chatbot/"><meta property="og:type" content="article"><link rel="preload" href="https://rainkiller981.github.io/Ricerca_su_Chatbot/assets/dynamic/fonts/jetbrainsmono/jetbrainsmono.woff2" as="font" type="font/woff2" crossorigin><link rel="stylesheet" href="https://rainkiller981.github.io/Ricerca_su_Chatbot/assets/css/style.css?v=7af13b1c052add3b4932cfce53d5f2fe"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://rainkiller981.github.io/Ricerca_su_Chatbot/software-per-la-gestione-dei-chatbot/"},"headline":"Software per la gestione dei Chatbot","datePublished":"2024-06-16T16:52","dateModified":"2024-06-17T10:59","description":"In questa sezione si analizzano tutti i software utilizzati per la ricerca, ossia BackyardAI, TavernAI e OobaBooga.","author":{"@type":"Person","name":"Andrea Pusceddu e Anna Maria Sanna","url":"https://rainkiller981.github.io/Ricerca_su_Chatbot/authors/andrea-pusceddu-e-anna-maria-sanna/"},"publisher":{"@type":"Organization","name":"Andrea Pusceddu e Anna Maria Sanna"}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body><div class="container"><header class="header"><div class="header__logo"><a class="logo" href="https://rainkiller981.github.io/Ricerca_su_Chatbot/">Chatbot</a></div><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu" aria-haspopup="true" aria-expanded="false"><span class="navbar__toggle-box"><span class="navbar__toggle-inner">Menu</span></span></button><ul class="navbar__menu"></ul></nav></header><main class="content"><article class="post"><header><h1 class="post__title">Software per la gestione dei Chatbot</h1><div class="post__meta"><time datetime="2024-06-16T16:52" class="post__date">domenica, 16 giugno 2024 </time><span class="post__author"><a href="https://rainkiller981.github.io/Ricerca_su_Chatbot/authors/andrea-pusceddu-e-anna-maria-sanna/" class="feed__author">Andrea Pusceddu e Anna Maria Sanna</a></span></div></header><div class="post__entry"><ul><li><a href="https://rainkiller981.github.io/Ricerca_su_Chatbot/cosa-e-backyardai/">Cosa è BackyardAI?</a></li><li><a href="https://rainkiller981.github.io/Ricerca_su_Chatbot/cosa-e-oobabooga/">Cosa è OobaBooga?</a></li><li><a href="https://rainkiller981.github.io/Ricerca_su_Chatbot/cosa-e-tavernai/">Cosa è TavernAI?</a></li></ul><h3 id="come-si-scaricano-le-piattaforme-e-i-programmi">Come si scaricano le piattaforme e i programmi</h3><p>In questa parte si tratterà il discorso dei prerequisiti per far funzionare le chatbot. Per ogni punto verranno elencati i programmi necessari, le stringhe di codice da utilizzare e il sistema più semplici che si è trovato per scaricare lo strumento da 0.</p><h3 id="oobabooga">Oobabooga</h3><p>Per quanto riguarda questa piattaforma, il sistema più semplice e più intuitivo, che utilizza programmi con possibili applicazioni future, è il download attraverso l’uso di Miniconda <sup class="footnote" id="fnref-1"><a href="https://it.wikipedia.org/wiki/Five_Nights_at_Freddy%27s" rel="footnote">1</a></sup></p><h4 id="passaggio-1">Passaggio 1</h4><p>Scaricare Miniconda attraverso l’installer che si può trovare sul sito <a href="https://docs.anaconda.com/free/miniconda/">https://docs.anaconda.com/free/miniconda/</a></p><h4 id="passaggio-2">Passaggio 2</h4><p>Aprire Miniconda e creare un nuovo ambiente impostando questi due comandi (uno alla volta) e premendo invio:</p><ul><li>conda create -n textgen python=3.11</li><li>conda activate textgen</li></ul><h4 id="passaggio-3">Passaggio 3</h4><p>Serve scaricare Pytorch <sup class="footnote" id="fnref-1"><a href="https://it.wikipedia.org/wiki/Five_Nights_at_Freddy%27s" rel="footnote">2</a></sup> inserendo, appena possibile, queste linee di codice in base al sistema operativo che ospiterà il programma. Prima di far partire questi comandi bisogna assicurarsi di aver caricato correttamente l’environment di Miniconda e vedere se esso è compatibile con il sistema in uso:</p><table><thead><tr><th>Sistema</th><th>Configurazione</th><th>Comando</th></tr></thead><tbody><tr><td><strong>Linux/WSL</strong></td><td><strong>NVIDIA</strong></td><td><code>pip3 install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121</code></td></tr><tr><td><strong>Linux/WSL</strong></td><td><strong>CPU only</strong></td><td><code>pip3 install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cpu</code></td></tr><tr><td><strong>Linux</strong></td><td><strong>AMD</strong></td><td><code>pip3 install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/rocm5.6</code></td></tr><tr><td><strong>MacOS + MPS</strong></td><td><strong>Any</strong></td><td><code>pip3 install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1</code></td></tr><tr><td><strong>Windows</strong></td><td><strong>NVIDIA</strong></td><td><code>pip3 install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121</code></td></tr><tr><td><strong>Windows</strong></td><td><strong>CPU only</strong></td><td><code>pip3 install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1</code></td></tr></tbody></table><p>Queste stringhe di codice sono al momento le più vecchie. Per ottenere quelle più recenti, bisogna andare sul sito <a href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a>.</p><p>Se si utilizzano sistemi NVIDIA che supportano il CUDA, bisogna scaricare la CUDA libraries:</p><ul><li>conda install -y -c “nvidia/label/cuda-12.1.1” cuda-runtime,</li></ul><p>altrimenti se si necessita di creare librerie personalizzate, bisogna utilizzare questo codice:</p><ul><li>conda install -y -c “nvidia/label/cuda-12.1.1” (qui si inserisce la libreria di riferimento) cuda</li></ul><h4 id="passaggio-4">Passaggio 4</h4><p>Una volta finito di scaricare il Pytorch, si passa alla parte dedicata alla UI, l’interfaccia utente. Si parte inserendo queste 3 stringhe di codice singolarmente, ma bisogna dare particolare attenzione all’ultima:</p><ul><li><p>git clone <a href="https://github.com/oobabooga/text-generation-webui">https://github.com/oobabooga/text-generation-webui</a>;</p></li><li><p>cd text-generation-webui;</p></li><li><p>pip install -r<file scaricabili in base alla tabella e alle componenti del sistema>;</file></p></li></ul><p>Pip install -r richiede una particolare attenzione, legata principalmente ai componenti della macchina. Si deve seguire questa tabella per ottenere il file corretto, altrimenti la macchina avrà problemi a scaricare, riconoscere o attivare il programma:</p><table><thead><tr><th>GPU</th><th>CPU</th><th>Requirements File</th></tr></thead><tbody><tr><td>NVIDIA</td><td>has AVX2</td><td>requirements.txt</td></tr><tr><td>NVIDIA</td><td>no AVX2</td><td>requirements_noavx2.txt</td></tr><tr><td>AMD</td><td>has AVX2</td><td>requirements_amd.txt</td></tr><tr><td>AMD</td><td>no AVX2</td><td>requirements_amd_noavx2.txt</td></tr><tr><td>CPU only</td><td>has AVX2</td><td>requirements_cpu_only.txt</td></tr><tr><td>CPU only</td><td>no AVX2</td><td>requirements_cpu_only_noavx2.txt</td></tr><tr><td>Apple</td><td>Intel</td><td>requirements_apple_intel.txt</td></tr><tr><td>Apple</td><td>Apple Silicon</td><td>requirements_apple_silicon.txt</td></tr></tbody></table><p>Tutti i requisiti saranno scaricabili direttamente dalla stringa di codice inserita in pip install. Particolare attenzione bisogna darla ai casi che:</p><ul><li>montano una GPU AMD su Windows: in questo bisogna eseguire tre passaggi in più.</li></ul><p>Il primo consiste nel scaricare il pip install -r solo con questi due codici, in base alle necessità:</p><ul><li>requirements_cpu_only.txt;</li><li>requirements_cpu_only_noavx2.txt.</li></ul><p>Il secondo è scaricare manualmente llama-cpp-python inserendo l’approppiato comando sull’hardware, seguendo la guida qui indicata: <a href="https://github.com/abetlen/llama-cpp-python#installation-with-hardware-acceleration">https://github.com/abetlen/llama-cpp-python#installation-with-hardware-acceleration</a> (io per questo passaggio non ho avuto necessità di seguire gli ultimi due punti).</p><p>Il terzo è scaricare manuale AUTOGPTQ seguendo le indicazioni che si possono trovare su <a href="https://github.com/AutoGPTQ/AutoGPTQ#install-from-source">https://github.com/AutoGPTQ/AutoGPTQ#install-from-source</a>.</p><ul><li>hanno vecchie GPU invidia, da Kepler in poi. Qui si richiede di scaricare CUDA 11.8 invece che 12:</li></ul><p>pip3 install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 –index-url <a href="https://download.pytorch.org/whl/cu118">https://download.pytorch.org/whl/cu118</a> conda install -y -c “nvidia/label/cuda-11.8.0” cuda-runtime</p><p>e gli si richiede anche versioni più vecchie di bitsandbytes 0.39, nel caso si debba usare –load-in-8bit:</p><ul><li>pip install <a href="https://github.com/jllllll/bitsandbytes-windows-webui/raw/main/bitsandbytes-0.38.1-py3-none-any.whl">https://github.com/jllllll/bitsandbytes-windows-webui/raw/main/bitsandbytes-0.38.1-py3-none-any.whl</a> (Windows);</li><li>pip install bitsandbytes==0.38.1 (Linux).</li></ul><p>Una volta scaricato bisogna controllare che il programma sia aggiornato, quindi selezionare update in base al sistema operativo e, se è aggiornato tutto, basterà premere start_(OS), per far partire il cmd con il link per la UI, solitamente indicata come <a href="http://localhost:7860/?%5C_%5C_theme=dark">http://localhost:7860/?\_\_theme=dark</a></p><h3 id="tavernai">TavernAi</h3><p>I passaggi per scaricare TavernAI sono molto più brevi, però richiedono un attenzione maggiore per evitare di cadere in fallo. Se non funziona l’installer one-click, allora si possono seguire i seguenti passaggi (obbligatorio sempre il passaggio 1 anche quando si usa l’installer base)</p><h4 id="passaggio-1-1">Passaggio 1</h4><p>Scaricare Node.js dal sito <a href="https://nodejs.org/en/download/package-manager/current">https://nodejs.org/en/download/package-manager/current</a>, selezionando tutti i checkbox e rispondendo Yes to All quando il tools lo richiede</p><h4 id="passaggio-2-1">Passaggio 2</h4><p>Scaricare TavernAI e farlo partire premendo:</p><ul><li>Start.bat se si trova su Windows;</li><li>Start-linux.sh se si trova su Linux;</li><li>npm install e node server.js (per attivare il server) su altri sistemi operativi;</li></ul><h4 id="passaggio-3-1">Passaggio 3</h4><p>Da questo momento in poi, TavernAi sarà funzionante solo in apparenza. Avrà bisogno di scaricare una backends.</p><p>Si seleziona la backends di riferimento e si va su Settings. Si seleziona il programma di riferimento (nel nostro caso Oobabooga, ma supporta anche Kobold Ai e NovelAi, insieme a molti altri<sup class="footnote" id="fnref-1"><a href="https://github.com/TavernAI/TavernAI" rel="footnote">3</a></sup>).</p><p>Si entra su Oobabooga e, in primis si carica LLM sul programma. In seguito, si entra nella pagina Session e si selezionano i riquadri “api” e “public_api” e si preme “Apply flags/extensions and restart”.</p><p>Si ritorna su TavernAi e si selezione “Text Generation Web Ui” e si inserisce <a href="http://127.0.0.1:5000">http://127.0.0.1:5000</a> oppure il link che compare sul cmd di OobaBooga. Se il lavoro risulta completo, comparirà la lucina verde con scritto “None” se non si è caricato nessun LLM, oppure il nome dell’LLM.</p><h3 id="backyardai">BackyardAI</h3><p>Il sistema si presenta come il più semplice in quanto richiede di scaricare l’installer su <a href="https://backyard.ai/">https://backyard.ai/</a> oppure di utilizzare il programma integrato nel browser.</p><p>Se si scarica il .exe, il programma stesso guiderà l’utente nella sua installazione.</p><h1 id="come-si-caricano-gli-llm">Come si caricano gli LLM</h1><p>Caricare gli LLM è il cuore di tutto il lavoro. Senza di essi, la ricerca non sarebbe possibili e grazie alla quantità indefinita di LLM che si stanno creando, sviluppando e installando, bisogna quantomeno conoscere come si installano</p><h2 id="oobabooga-1">OobaBooga</h2><p>Per prima cosa bisogna aprirà Oobabooga.</p><p>Una volta entrato nell’interfaccia, bisogna entrare nella sezione model e si cerca la sezione “Download”.</p><p>Per scaricare LoRA e LLM, questa sezione richiederà i link delle pagine web dedicate. Un esempio di link che si deve utilizzare è <a href="https://huggingface.co/stablediffusionapi/afrodite-xl-v2">https://huggingface.co/stablediffusionapi/afrodite-xl-v2</a>, da cui si potrà ottenere il pacchetto LLM conosciuto come Afrodite.</p><p>Si inserisce il link e si preme download, in sottofondo, nel cmd, si vedrà lo scaricamento del pacchetto in background.</p><p>Questa opzione non funziona soltanto con Huggingface, ma sfrutta anche siti come Github.</p><h2 id="backyardai-1">BackyardAI</h2><p>Sistema molto simile al precedente, BackyardAI sfrutta due sistemi per installare gli LLM.</p><p>In primis ripropone la possibilità di sfruttare i link per facilitare il ritrovamento dei pacchetti LLM.</p><p>Altrimenti in Manage Models, Backyard presenta un catalogo di LLM già preselezionato e aggiornato che permette di scaricare i pacchetti in maniera veloce.</p><h3 id="note">Note</h3><ol><li><sup class="footnote" id="fnref-1"><a href="https://github.com/oobabooga/text-generation-webui?tab=readme-ov-file#one-click-installer" rel="footnote"></a></sup>Tutorial ripreso da <a href="https://github.com/oobabooga/text-generation-webui?tab=readme-ov-file#one-click-installer">https://github.com/oobabooga/text-generation-webui?tab=readme-ov-file#one-click-installer</a>. Tabelle riproposte in Markdown utilizzando il supporto di ChatGPT</li><li><sup class="footnote" id="fnref-1"><a href="https://en.wikipedia.org/wiki/PyTorch" rel="footnote"></a></sup><a href="https://en.wikipedia.org/wiki/PyTorch">https://en.wikipedia.org/wiki/PyTorch</a></li><li><sup class="footnote" id="fnref-1"><a href="**https://github.com/TavernAI/TavernAI**" rel="footnote"></a></sup><a href="https://github.com/TavernAI/TavernAI">https://github.com/TavernAI/TavernAI</a></li></ol></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on lunedì, 17 giugno 2024</p><div class="post__share"></div></footer></article></main><footer class="footer"><div class="footer__inner"><div class="footer__copyright"><p>© 2024 Powered by Publii CMS :: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank" rel="noopener">Theme</a> ported by the <a href="https://getpublii.com/customization-service/" target="_blank" rel="noopener">Publii Team</a></p></div></div></footer></div><script defer="defer" src="https://rainkiller981.github.io/Ricerca_su_Chatbot/assets/js/scripts.min.js?v=74fad06980c30243d91d72c7c57fcdb8"></script><script>window.publiiThemeMenuConfig={mobileMenuMode:'overlay',animationSpeed:300,submenuWidth: 'auto',doubleClickTime:500,mobileMenuExpandableSubmenus:true,relatedContainerForOverlayMenuSelector:'.top'};</script><script>var images = document.querySelectorAll('img[loading]');
        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>